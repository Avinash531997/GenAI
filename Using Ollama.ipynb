{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c957f2-e69c-4049-855f-1785de44ce17",
   "metadata": {},
   "source": [
    "#### Visit ollama.com and install - Ollama. On completion, the ollama server starts running locally @ http://localhost:11434/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d850f0-7057-45ae-b835-da036433a764",
   "metadata": {},
   "source": [
    "#### Process 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05942dc9-a685-4dcb-90f5-e63821bcc76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee05efbb-8cd8-4b8f-94b9-b6bebb468a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "#!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5817f812-2bc6-44ab-8c67-e977312e3eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers architecture consists of an encoder and decoder. The encoder takes input sequences (e.g., text, images) and outputs a sequence of attention weights and values. The decoder receives these weights and outputs the final output sequence, using self-attention mechanisms to weigh importance and position dependencies within the input sequence.\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# Create a messages list using the same format that we used for OpenAI\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain transformers architecture in  50 words\"}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505aad0-fd10-4e50-9762-aa4125e0e0e3",
   "metadata": {},
   "source": [
    "#### Process 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eeb1298-0837-4b02-bd02-4969151e275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers is a neural network architecture that processes sequential data. It uses self-attention mechanisms to weigh importance of input elements when generating output. The key components are: (1) Encoder: transforms input sequence, and (2) Decoder: generates output sequence based on the encoded input sequence's representation.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ddf51-e867-4d80-910e-d08d63eddcd7",
   "metadata": {},
   "source": [
    "#### Process 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9a50744-a4ce-4e73-a44f-741bda3f7c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers is a neural network architecture that uses self-attention to process sequential data. It consists of an encoder and decoder, with attention weights learning the relationship between input tokens. The architecture allows for parallel processing and has been successful in image classification, natural language processing, and other applications.\n"
     ]
    }
   ],
   "source": [
    "#Using the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
